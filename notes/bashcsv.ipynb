{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV command-line kung fu\n",
    "\n",
    "You might be surprised how much data slicing and dicing you can do from the command line using some simple tools and I/O redirection + piping. (See [A Quick Introduction to Pipes and Redirection](http://bconnelly.net/working-with-csvs-on-the-command-line/#a-quick-introduction-to-pipes-and-redirection)). We've already seen I/O redirection where we took the output of a command and wrote it to a file (`/tmp/t.csv`):\n",
    " \n",
    "```bash\n",
    "$ iconv -c -f utf-8 -t ascii SampleSuperstoreSales.csv > /tmp/t.csv\n",
    "```\n",
    "\n",
    "### Extracting rows\n",
    "\n",
    "Now, let me introduce you to the `grep` command that lets us filter the lines in a file according to a regular expression. Here's how to find all rows that contain `Annie Cyprus`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249,1702,5/6/11,High,23,67.24,0.06,Regular Air,4.90,2.84,0.93,Annie Cyprus,Nunavut,Nunavut,Home Office,Office Supplies,Pens & Art Supplies,SANFORD Liquid Accent� Tank-Style Highlighters,Wrap Bag,0.54,5/7/11\r\n",
      "669,4676,8/31/11,High,11,1210.0515,0.04,Regular Air,-104.25,125.99,7.69,Annie Cyprus,Nunavut,Nunavut,Home Office,Technology,Telephones and Communication,Timeport L7089,Small Box,0.58,9/1/11\r\n",
      "670,4676,8/31/11,High,50,187.83,0.03,Regular Air,85.96,3.75,0.5,Annie Cyprus,Nunavut,Nunavut,Home Office,Office Supplies,Labels,Avery 510,Small Box,0.37,9/2/11\r\n",
      "671,4676,8/31/11,High,3,49.59,0.07,Express Air,-8.38,12.28,6.47,Annie Cyprus,Nunavut,Nunavut,Home Office,Office Supplies,Paper,Xerox 1881,Small Box,0.38,9/2/11\r\n",
      "672,4676,8/31/11,High,30,4253.009,0.01,Regular Air,1115.69,155.99,8.99,Annie Cyprus,Nunavut,Nunavut,Home Office,Technology,Telephones and Communication,LX 788,Small Box,0.58,9/1/11\r\n",
      "734,5284,7/8/11,Not Specified,7,59.38,0.1,Regular Air,-3.05,8.69,2.99,Annie Cyprus,Nunavut,Nunavut,Home Office,Office Supplies,Binders and Binder Accessories,\"Cardinal Slant-D� Ring Binder, Heavy Gauge Vinyl\",Small Box,0.39,7/10/11\r\n",
      "3643,26051,6/27/10,Not Specified,22,795.74,0,Delivery Truck,-127.39,33.94,19.19,Annie Cyprus,Northwest Territories,Northwest Territories,Home Office,Furniture,Chairs & Chairmats,\"Metal Folding Chairs, Beige, 4/Carton\",Jumbo Drum,0.58,6/29/10\r\n",
      "3644,26051,6/27/10,Not Specified,31,251.75,0.07,Regular Air,22.46,8.33,1.99,Annie Cyprus,Northwest Territories,Northwest Territories,Home Office,Technology,Computer Peripherals,\"80 Minute Slim Jewel Case CD-R , 10/Pack - Staples\",Small Pack,0.52,6/28/10\r\n"
     ]
    }
   ],
   "source": [
    "! grep 'Annie Cyprus' data/SampleSuperstoreSales.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't have to write any code. We didn't have to jump into a development environment or editor. We just asked for the sales for Annie. If we want to write the data to a file, we simply redirect it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249,1702,5/6/11,High,23,67.24,0.06,Regular Air,4.90,2.84,0.93,Annie Cyprus,Nunavut,Nunavut,Home Office,Office Supplies,Pens & Art Supplies,SANFORD Liquid Accent� Tank-Style Highlighters,Wrap Bag,0.54,5/7/11\r\n",
      "669,4676,8/31/11,High,11,1210.0515,0.04,Regular Air,-104.25,125.99,7.69,Annie Cyprus,Nunavut,Nunavut,Home Office,Technology,Telephones and Communication,Timeport L7089,Small Box,0.58,9/1/11\r\n",
      "670,4676,8/31/11,High,50,187.83,0.03,Regular Air,85.96,3.75,0.5,Annie Cyprus,Nunavut,Nunavut,Home Office,Office Supplies,Labels,Avery 510,Small Box,0.37,9/2/11\r\n",
      "671,4676,8/31/11,High,3,49.59,0.07,Express Air,-8.38,12.28,6.47,Annie Cyprus,Nunavut,Nunavut,Home Office,Office Supplies,Paper,Xerox 1881,Small Box,0.38,9/2/11\r\n",
      "672,4676,8/31/11,High,30,4253.009,0.01,Regular Air,1115.69,155.99,8.99,Annie Cyprus,Nunavut,Nunavut,Home Office,Technology,Telephones and Communication,LX 788,Small Box,0.58,9/1/11\r\n",
      "734,5284,7/8/11,Not Specified,7,59.38,0.1,Regular Air,-3.05,8.69,2.99,Annie Cyprus,Nunavut,Nunavut,Home Office,Office Supplies,Binders and Binder Accessories,\"Cardinal Slant-D� Ring Binder, Heavy Gauge Vinyl\",Small Box,0.39,7/10/11\r\n",
      "3643,26051,6/27/10,Not Specified,22,795.74,0,Delivery Truck,-127.39,33.94,19.19,Annie Cyprus,Northwest Territories,Northwest Territories,Home Office,Furniture,Chairs & Chairmats,\"Metal Folding Chairs, Beige, 4/Carton\",Jumbo Drum,0.58,6/29/10\r\n",
      "3644,26051,6/27/10,Not Specified,31,251.75,0.07,Regular Air,22.46,8.33,1.99,Annie Cyprus,Northwest Territories,Northwest Territories,Home Office,Technology,Computer Peripherals,\"80 Minute Slim Jewel Case CD-R , 10/Pack - Staples\",Small Pack,0.52,6/28/10\r\n"
     ]
    }
   ],
   "source": [
    "! grep 'Annie Cyprus' data/SampleSuperstoreSales.csv > /tmp/Annie.csv\n",
    "! head /tmp/Annie.csv # show first few lines of that new file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want a specific row ID, we use a different regular expression that looks for a specific string at the left edge of a line (`^` means the beginning of a line):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80,483,7/10/11,High,30,4965.7595,0.08,Regular Air,1198.97,195.99,3.99,Clay Rozendal,Nunavut,Nunavut,Corporate,Technology,Telephones and Communication,R380,Small Box,0.58,7/12/11\r\n"
     ]
    }
   ],
   "source": [
    "! grep '^80,' data/SampleSuperstoreSales.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you want, say, two different rows added to another file?  We do two `grep`s and a `>>` concatenation redirection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80,483,7/10/11,High,30,4965.7595,0.08,Regular Air,1198.97,195.99,3.99,Clay Rozendal,Nunavut,Nunavut,Corporate,Technology,Telephones and Communication,R380,Small Box,0.58,7/12/11\r\n",
      "160,995,5/30/11,Medium,46,1815.49,0.03,Regular Air,782.91,39.89,3.04,Neola Schneider,Nunavut,Nunavut,Home Office,Furniture,Office Furnishings,Ultra Commercial Grade Dual Valve Door Closer,Wrap Bag,0.53,5/31/11\r\n"
     ]
    }
   ],
   "source": [
    "! grep '^80,' data/SampleSuperstoreSales.csv > /tmp/two.csv # write first row\n",
    "! grep '^160,' data/SampleSuperstoreSales.csv >> /tmp/two.csv # append second row\n",
    "! cat /tmp/two.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we'd like to see just the header row, we can use `head`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row ID,Order ID,Order Date,Order Priority,Order Quantity,Sales,Discount,Ship Mode,Profit,Unit Price,Shipping Cost,Customer Name,Province,Region,Customer Segment,Product Category,Product Sub-Category,Product Name,Product Container,Product Base Margin,Ship Date\r\n"
     ]
    }
   ],
   "source": [
    "! head -1 data/SampleSuperstoreSales.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If, on the other hand, we want to see everything but that row, we can use `tail` (which I pipe to `head` so then I see only the first two lines of output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,3,10/13/10,Low,6,261.54,0.04,Regular Air,-213.25,38.94,35,Muhammed MacIntyre,Nunavut,Nunavut,Small Business,Office Supplies,Storage & Organization,\"Eldon Base for stackable storage shelf, platinum\",Large Box,0.8,10/20/10\r\n",
      "49,293,10/1/12,High,49,10123.02,0.07,Delivery Truck,457.81,208.16,68.02,Barry French,Nunavut,Nunavut,Consumer,Office Supplies,Appliances,\"1.7 Cubic Foot Compact \"\"Cube\"\" Office Refrigerators\",Jumbo Drum,0.58,10/2/12\r\n",
      "tail: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "! tail +2 data/SampleSuperstoreSales.csv | head -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output would normally be many thousands of lines here so I have *piped* the output to the `head` command to print just the first two rows.  We can pipe many commands together, sending the output of one command as input to the next command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Count how many sales items there are in the `Technology` product category that are also `High` order priorities? Hint: `wc -l` counts the number of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     451\r\n"
     ]
    }
   ],
   "source": [
    "! grep 'Technology' data/SampleSuperstoreSales.csv | grep High | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Extracting columns\n",
    "\n",
    "Extracting columns is also pretty easy as long as there is a single delimiter, such as a comma, that clearly separates the columns. For example, let's say we wanted to get the customer name column (which is 12th by my count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Name\r\n",
      "Muhammed MacIntyre\r\n",
      "Barry French\r\n",
      "Barry French\r\n",
      "Clay Rozendal\r\n",
      "Carlos Soltero\r\n",
      "Carlos Soltero\r\n",
      "Carl Jackson\r\n",
      "Carl Jackson\r\n",
      "Monica Federle\r\n"
     ]
    }
   ],
   "source": [
    "! cut -d ',' -f 12 /tmp/t.csv | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Where I'm using the `iconv`erted `/tmp/t.csv` file as `cut` expects pure ascii.) Actually, hang on a second. We don't want the `Customer Name` header to appear in the list so we combined with the `tail` we just saw to strip the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muhammed MacIntyre\r\n",
      "Barry French\r\n",
      "Barry French\r\n",
      "Clay Rozendal\r\n",
      "Carlos Soltero\r\n",
      "Carlos Soltero\r\n",
      "Carl Jackson\r\n",
      "Carl Jackson\r\n",
      "Monica Federle\r\n",
      "Dorothy Badders\r\n",
      "tail: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "! cut -d ',' -f 12 /tmp/t.csv | tail +2 | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want a unique list? All we have to do is sort and then call `uniq`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aaron Bergman\r\n",
      "Aaron Hawkins\r\n",
      "Aaron Smayling\r\n",
      "Adam Bellavance\r\n",
      "Adam Hart\r\n",
      "Adam Shillingsburg\r\n",
      "Adrian Barton\r\n",
      "Adrian Hane\r\n",
      "Adrian Shami\r\n",
      "Aimee Bixby\r\n"
     ]
    }
   ],
   "source": [
    "! cut -d ',' -f 12 /tmp/t.csv | tail +2 | sort | uniq | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get multiple columns at once but the order is always left to right (`cut` does not know how to flip column order). For example, here is how to get the sales ID and the customer name together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order ID,Customer Name\r\n",
      "3,Muhammed MacIntyre\r\n",
      "293,Barry French\r\n",
      "293,Barry French\r\n",
      "483,Clay Rozendal\r\n",
      "515,Carlos Soltero\r\n",
      "515,Carlos Soltero\r\n",
      "613,Carl Jackson\r\n",
      "613,Carl Jackson\r\n",
      "643,Monica Federle\r\n"
     ]
    }
   ],
   "source": [
    "! cut -d ',' -f 2,12 /tmp/t.csv |head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, we can write any of this output to a file using the `>` redirection operator. Let's do that and put each of those columns into a separate file and then `paste` them back with the customer name first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Name\tOrder ID\r\n",
      "Muhammed MacIntyre\t3\r\n",
      "Barry French\t293\r\n",
      "Barry French\t293\r\n",
      "Clay Rozendal\t483\r\n",
      "Carlos Soltero\t515\r\n",
      "Carlos Soltero\t515\r\n",
      "Carl Jackson\t613\r\n",
      "Carl Jackson\t613\r\n",
      "Monica Federle\t643\r\n"
     ]
    }
   ],
   "source": [
    "! cut -d ',' -f 2 /tmp/t.csv > /tmp/IDs\n",
    "! cut -d ',' -f 12 /tmp/t.csv > /tmp/names\n",
    "! paste /tmp/names /tmp/IDs | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing, right?! This is often a very efficient means of manipulating data files because you are directly talking to the operating system instead of through Python libraries. We also don't have to write any code, we just have to know some syntax for terminal commands.\n",
    "\n",
    "Not impressed yet? Ok, how about creating a histogram indicating the number of sales per customer sorted in reverse numerical order? We've already get list of customers, but we can use an argument on `uniq` to get the count instead of just making a unique set. Then, we can use a second `sort` with arguments to reverse sort and use numeric rather than text-based sorting. This gives us a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  41 Darren Budd\r\n",
      "  38 Ed Braxton\r\n",
      "  35 Brad Thomas\r\n",
      "  33 Carlos Soltero\r\n",
      "  30 Patrick Jones\r\n",
      "  29 Tony Sayre\r\n",
      "  28 Nora Price\r\n",
      "  28 Mark Cousins\r\n",
      "  28 Lena Creighton\r\n",
      "  28 Joy Smith\r\n"
     ]
    }
   ],
   "source": [
    "! cut -d ',' -f 12 /tmp/t.csv | tail +2 | sort | uniq -c | sort -r -n | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Modify the command so that you get a histogram of the shipping mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6270 Regular Air\r\n",
      "1146 Delivery Truck\r\n",
      " 983 Express Air\r\n"
     ]
    }
   ],
   "source": [
    "! cut -d ',' -f 8 /tmp/t.csv | tail +2 | sort | uniq -c | sort -r -n | head -10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "103px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
